{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings.\n",
      "(997L, 128L)\n",
      "(997L, 1L) (997L, 128L)\n",
      "----------------------------------------------------\n",
      "[80,20] [Train,Test] Split\n",
      "Loading embeddings.\n",
      "Training for 50 classes.\n",
      "\tTesting error is 0.03\n",
      "----------------------------------------------------\n",
      "----------------------------------------------------\n",
      "[60,40] [Train,Test] Split\n",
      "Loading embeddings.\n",
      "Training for 50 classes.\n",
      "\tTesting error is 0.0501253132832\n",
      "----------------------------------------------------\n",
      "----------------------------------------------------\n",
      "[50,50] [Train,Test] Split\n",
      "Loading embeddings.\n",
      "Training for 50 classes.\n",
      "\tTesting error is 0.0681362725451\n",
      "----------------------------------------------------\n",
      "----------------------------------------------------\n",
      "[40,60] [Train,Test] Split\n",
      "Loading embeddings.\n",
      "Training for 50 classes.\n",
      "\tTesting error is 0.0684474123539\n",
      "----------------------------------------------------\n",
      "----------------------------------------------------\n",
      "[20,80] [Train,Test] Split\n",
      "Loading embeddings.\n",
      "Training for 50 classes.\n",
      "\tTesting error is 0.209273182957\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "#\n",
    "# Example to classify faces.\n",
    "# Brandon Amos\n",
    "# 2015/10/11\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "import argparse\n",
    "#import cv2\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "import pandas as pd\n",
    "\n",
    "#import openface\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split,  KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#fileDir = os.path.dirname(os.path.realpath(__file__))\n",
    "#modelDir = os.path.join(fileDir, '..', 'models')\n",
    "#dlibModelDir = os.path.join(modelDir, 'dlib')\n",
    "#openfaceModelDir = os.path.join(modelDir, 'openface')\n",
    "\n",
    "\n",
    "def train(classfier, data, labelsNum, nClasses,):\n",
    "    print(\"Loading embeddings.\")\n",
    "    fname = \"{}/labels.csv\".format(workDir)\n",
    "    labels = data[:,0]\n",
    "    embeddings = data[:,1:]\n",
    "    labelsNum = labels.tolist()\n",
    "    print(\"Training for {} classes.\".format(nClasses))\n",
    "    if classifier == 'LinearSvm':\n",
    "        clf = SVC(C=1, kernel='linear', probability=True)\n",
    "    elif classifier == 'GridSearchSvm':\n",
    "        print(\"\"\"\n",
    "        Warning: In our experiences, using a grid search over SVM hyper-parameters only\n",
    "        gives marginally better performance than a linear SVM with C=1 and\n",
    "        is not worth the extra computations of performing a grid search.\n",
    "        \"\"\")\n",
    "        param_grid = [\n",
    "            {'C': [1, 10, 100, 1000],\n",
    "             'kernel': ['linear']},\n",
    "            {'C': [1, 10, 100, 1000],\n",
    "             'gamma': [0.001, 0.0001],\n",
    "             'kernel': ['rbf']}\n",
    "        ]\n",
    "        clf = GridSearchCV(SVC(C=1, probability=True), param_grid, cv=5)\n",
    "    # ref:\n",
    "    # http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#example-classification-plot-classifier-comparison-py\n",
    "    elif classifier == 'DecisionTree':  # Doesn't work best\n",
    "        clf = DecisionTreeClassifier(max_depth=20)\n",
    "    elif classifier == 'KNN':\n",
    "        clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    elif classifier == 'AdaBoost':\n",
    "        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=20), n_estimators=100)\n",
    "    elif classifier == 'RandomForest':\n",
    "        clf = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "    clf.fit(embeddings, labelsNum)\n",
    "\n",
    "    \"\"\"\n",
    "    ### saving model to local file\n",
    "    fName = \"{}/classifier.pkl\".format(workDir)\n",
    "    print(\"Saving classifier to '{}'\".format(fName))\n",
    "    with open(fName, 'w') as f:\n",
    "        pickle.dump((le, clf), f)\n",
    "    \"\"\"\n",
    "    \n",
    "    return clf\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def infer(clf, X, Y, multiple=False, verbose=True):\n",
    "    \"\"\"\n",
    "    classifierModel = \"{}/classifier.pkl\".format(workDir)\n",
    "    with open(classifierModel, 'rb') as f:\n",
    "        if sys.version_info[0] < 3:\n",
    "                (le, clf) = pickle.load(f)\n",
    "        else:\n",
    "                (le, clf) = pickle.load(f, encoding='latin1')\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO Store testing represenations in folder \n",
    "    f_x = clf.predict(X)\n",
    "    error = np.sum(Y[:,0] != f_x) / float(len(Y))\n",
    "\n",
    "    print \"\\tTesting error is {}\".format(error)\n",
    "    return error\n",
    "    # reps = getRep(img, multiple)\n",
    "    # if len(reps) > 1:\n",
    "    #     print(\"List of faces in image from left to right\")\n",
    "    # for r in reps:\n",
    "    #     rep = r[1].reshape(1, -1)\n",
    "    #     bbx = r[0]\n",
    "\n",
    "    #     start = time.time()\n",
    "    #     predictions = clf.predict_proba(rep).ravel()\n",
    "    #     maxI = np.argmax(predictions)\n",
    "    #     person = le.inverse_transform(maxI)\n",
    "    #     confidence = predictions[maxI]\n",
    "\n",
    "    #     if verbose:\n",
    "    #         print(\"Prediction took {} seconds.\".format(time.time() - start))\n",
    "    #     if multiple:\n",
    "    #         print(\"Predict {} @ x={} with {:.2f} confidence.\".format(person.decode('utf-8'), bbx,\n",
    "    #                                                                  confidence))\n",
    "    #     else:\n",
    "    #         print(\"Predict {} with {:.2f} confidence.\".format(person.decode('utf-8'), confidence))\n",
    "\n",
    "    #     # match prediction with label\n",
    "    #     # Sum up \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    workDir = \"./training-embeddings\"\n",
    "    print(\"Loading embeddings.\")\n",
    "    fname = \"{}/labels.csv\".format(workDir)\n",
    "    labels = pd.read_csv(fname, header=None).as_matrix()[:, 0:1]\n",
    "    fname = \"{}/reps.csv\".format(workDir)\n",
    "    embeddings = pd.read_csv(fname, header=None).as_matrix()\n",
    "    le = LabelEncoder().fit(labels)\n",
    "    labelsNum = le.transform(labels)\n",
    "    nClasses = len(le.classes_)\n",
    "\n",
    "    print embeddings.shape\n",
    "    print labels.shape, embeddings.shape\n",
    "    data = np.append(labels,embeddings,axis=1)\n",
    "    Y = data[:,0:1]\n",
    "    X = data[:,1:]\n",
    "\n",
    "    # Split dataset\n",
    "    # Train on generated embeddings\n",
    "    splits = [.20,.40,.50,.60,.80]  # percentage of test set\n",
    "    clf_list = ['LinearSvm', 'DecisionTree', 'KNN', 'AdaBoost', 'RandomForest']\n",
    "    for split in splits:\n",
    "        print \"----------------------------------------------------\"\n",
    "        print \"[{},{}] [Train,Test] Split\".format(int(100-(split*100)),\\\n",
    "                                                int((split*100)))\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,Y,\n",
    "                                      test_size=split,random_state=42)\n",
    "        classifier = clf_list[2]\n",
    "        data = np.append(Y_train, X_train, axis=1)\n",
    "        clf = train(classifier, data, labelsNum, nClasses)\n",
    "\n",
    "        test_error = infer(clf, X_test, Y_test)\n",
    "        print \"----------------------------------------------------\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "\n",
    "# KNN\n",
    "\n",
    "def l2dist(p1, p2):\n",
    "    diff = p1 - p2\n",
    "    l2norm = np.linalg.norm(diff)\n",
    "    return l2norm\n",
    "\n",
    "def predict(xtrain, ytrain, point, k):\n",
    "    # get l2 distances\n",
    "    l2norms = []\n",
    "    for i in range(len(ytrain)):\n",
    "        l2norms.append([l2dist(xtrain[i,:], point), i])\n",
    "    # sort and get the first k points and their indices\n",
    "    l2norms = sorted(l2norms, key=itemgetter(0))\n",
    "    #print l2norms\n",
    "    near_points = l2norms[0:k]\n",
    "    near_indices = [point[1] for point in near_points]\n",
    "    near_labels = ytrain[near_indices].tolist()\n",
    "    #print near_labels\n",
    "    label = Counter(near_labels).most_common(1)[0][0]\n",
    "    #print label\n",
    "    return label\n",
    "\n",
    "def e_rate(y_orig, y_preds):\n",
    "    e_count = 0\n",
    "    for i in range(len(y_orig)):\n",
    "        if y_orig[i] != y_preds[i]:\n",
    "            e_count += 1\n",
    "    e = float(e_count) / len(y_orig)\n",
    "    return e\n",
    "\n",
    "def knn(xtrain, ytrain, X, Y, k):\n",
    "    # loop through all points for this k value\n",
    "    lb_list = []\n",
    "    for i in range(len(Y)):\n",
    "        point = X[i,:].reshape(1,-1)\n",
    "        lb = predict(xtrain, ytrain, point, k)\n",
    "        lb_list.append(lb)\n",
    "        #break\n",
    "    ypreds = np.asarray(lb_list)\n",
    "    #print ypreds\n",
    "    error_rate = e_rate(Y, ypreds)\n",
    "    #print error_rate\n",
    "    return error_rate\n",
    "\n",
    "\"\"\"\n",
    "Use cross validation to determine best k\n",
    "\"\"\"\n",
    "# X_train, Y_train are the pre-partitioned train set\n",
    "# X_train shape should be (num of samples) x (num of variables)\n",
    "k_values = [1, 3, 5, 7]\n",
    "n_fold = 5\n",
    "val_errors = []\n",
    "for k in k_values:\n",
    "    kf = KFold(n_splits=n_fold)\n",
    "    val_e = 0\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        trainx, valx = X_train[train_index], X_train[val_index]\n",
    "        trainy, valy = Y_train[train_index], Y_train[val_index]\n",
    "        val_e += knn(trainx, trainy, valx, valy, k) / float(n_fold)\n",
    "    val_errors.append(val_e)\n",
    "    print 'Current k: {}'.format(k)\n",
    "    print 'Val error: {}'.format(val_e)\n",
    "opt_idx = np.argmin(val_errors)\n",
    "best_k = k_values[opt_idx]\n",
    "print 'Best k value: {}\\n'.format(best_k)\n",
    "\n",
    "\"\"\"\n",
    "Determine train, val and test errors\n",
    "\"\"\"\n",
    "k = 1\n",
    "train_e = knn(X_train, Y_train, X_train, Y_train, k)\n",
    "print 'Training error: {}'.format(train_e)\n",
    "#val_e = knn(xtrain, ytrain, xval, yval, k)\n",
    "#print 'Validation error: {}'.format(val_e)\n",
    "test_e = knn(X_train, Y_train, X_test, Y_test, k)\n",
    "print 'Test error: {}'.format(test_e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
